{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load DeepLabV3 Pretrained Model"
      ],
      "metadata": {
        "id": "xvdKu_2hUa3x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNLSHuDlScNX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.segmentation import deeplabv3_resnet18\n",
        "\n",
        "# Load pretrained model\n",
        "model = deeplabv3_resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all layers (feature extraction)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the classifier head (to allow training just on the final layer)\n",
        "model.classifier[4] = torch.nn.Conv2d(\n",
        "    in_channels=256,\n",
        "    out_channels=1,  # 1 class (foreground: cell vs. background)\n",
        "    kernel_size=1\n",
        ")\n",
        "\n",
        "# Make the new layer trainable\n",
        "for param in model.classifier[4].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Stained Images and Prepare DataLoader\n"
      ],
      "metadata": {
        "id": "oVAbVqu4U_uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define Dataset class\n",
        "class StainedCellDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_files = os.listdir(root_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Define simple transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = StainedCellDataset(root_dir='/content/drive/MyDrive/SegmentationProject/data/stained_images', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "xPSVKg4BVA3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model on the Stained Images\n"
      ],
      "metadata": {
        "id": "qgBPhiMqVnb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.classifier[4].parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images in dataloader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Ground truth is just the stained images themselves\n",
        "        labels = (images.mean(dim=1, keepdim=True) > 0.5).float()  # Create dummy masks\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)['out']\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "aypCFZl0VoH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict and Visualize Segmentation Masks\n"
      ],
      "metadata": {
        "id": "d1xdBq99V7rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images in dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)['out']\n",
        "        preds = torch.sigmoid(outputs)\n",
        "        preds = (preds > 0.5).float()\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            axs[0].imshow(images[i].cpu().permute(1, 2, 0))\n",
        "            axs[0].set_title('Original Image')\n",
        "            axs[1].imshow(preds[i][0].cpu(), cmap='gray')\n",
        "            axs[1].set_title('Predicted Mask')\n",
        "            plt.show()\n",
        "        break  # Show only one batch\n"
      ],
      "metadata": {
        "id": "nEavapD7V7-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Predicted Segmentation Masks to Drive\n"
      ],
      "metadata": {
        "id": "tl9CBiqPXc3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Create directory to save masks if not exists\n",
        "save_dir = '/content/drive/MyDrive/SegmentationProject/data/predicted_masks'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, images in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)['out']\n",
        "        preds = torch.sigmoid(outputs)\n",
        "        preds = (preds > 0.5).float()\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            mask = preds[i][0].cpu()\n",
        "            filename = f\"mask_{idx}_{i}.png\"\n",
        "            TF.to_pil_image(mask).save(os.path.join(save_dir, filename))\n"
      ],
      "metadata": {
        "id": "zxhz_5DJXh74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Ground Truth by Counting Cells in Predicted Masks\n"
      ],
      "metadata": {
        "id": "8ZQMms-xZiDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Path to saved masks\n",
        "mask_dir = '/content/drive/MyDrive/SegmentationProject/data/predicted_masks'\n",
        "\n",
        "# List all mask files\n",
        "mask_files = sorted(os.listdir(mask_dir))\n",
        "\n",
        "# List to save cell counts\n",
        "cell_counts = []\n",
        "\n",
        "for mask_file in mask_files:\n",
        "    mask_path = os.path.join(mask_dir, mask_file)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Threshold to make sure mask is binary\n",
        "    _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Find connected components (each component = one cell)\n",
        "    num_labels, labels = cv2.connectedComponents(binary_mask)\n",
        "\n",
        "    # Subtract 1 because background is also counted\n",
        "    cell_count = num_labels - 1\n",
        "    cell_counts.append((mask_file, cell_count))\n",
        "\n",
        "# Print results\n",
        "for filename, count in cell_counts:\n",
        "    print(f\"{filename}: {count} cells\")\n"
      ],
      "metadata": {
        "id": "XSz-RFtuZibA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction Part\n"
      ],
      "metadata": {
        "id": "8W5busi0adR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1â€“2: Load Original Images and Match with GT Cell Counts\n"
      ],
      "metadata": {
        "id": "LJqZp2-NasG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/content/drive/MyDrive/SegmentationProject/data/original_images\n"
      ],
      "metadata": {
        "id": "V8A2FyMbarMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Path to original images\n",
        "original_dir = '/content/drive/MyDrive/SegmentationProject/data/original_images'\n",
        "original_files = sorted(os.listdir(original_dir))\n",
        "\n",
        "# GT from previous step: dict of {basename: count}\n",
        "gt_dict = {name.replace('mask', 'orig').replace('.png', '.png'): count for name, count in cell_counts}\n",
        "\n",
        "# Define transform (resize + tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Build dataset as list of (image, target)\n",
        "image_targets = []\n",
        "for filename in original_files:\n",
        "    path = os.path.join(original_dir, filename)\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    image = transform(image)\n",
        "\n",
        "    count = gt_dict.get(filename, 0)\n",
        "    image_targets.append((image, torch.tensor([count], dtype=torch.float)))\n"
      ],
      "metadata": {
        "id": "1zrrHir5a4M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Build Dataset and DataLoader for Regression\n"
      ],
      "metadata": {
        "id": "gXzmFZw9a4wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define Dataset class for regression\n",
        "class CellCountDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data  # list of (image, target)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, target = self.data[idx]\n",
        "        return image, target\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "regression_dataset = CellCountDataset(image_targets)\n",
        "regression_dataloader = DataLoader(regression_dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "uerrNcjba7nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Build a Simple CNN Model for Cell Count Regression\n"
      ],
      "metadata": {
        "id": "004vWJknbQTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple CNN for regression\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1))\n",
        "        )\n",
        "        self.regressor = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.regressor(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate model\n",
        "regression_model = SimpleCNN().to(device)\n"
      ],
      "metadata": {
        "id": "E5ndl79obs4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Define Loss Function and Optimizer for Regression\n"
      ],
      "metadata": {
        "id": "U1az_vZab8eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function (Mean Squared Error)\n",
        "regression_criterion = nn.MSELoss()\n",
        "\n",
        "# Define optimizer (Adam)\n",
        "regression_optimizer = optim.Adam(regression_model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "vJIB0FFZcArJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6: Train the CNN Model to Predict Cell Counts\n"
      ],
      "metadata": {
        "id": "ft53F1ckcCSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "regression_model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, targets in regression_dataloader:\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = regression_model(images)\n",
        "        loss = regression_criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        regression_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        regression_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(regression_dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "G5GQkH7ncHna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 7: Evaluate Model and Plot Predictions vs Ground Truth\n",
        "\n"
      ],
      "metadata": {
        "id": "ajYhvBI0cbSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "regression_model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in regression_dataloader:\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = regression_model(images)\n",
        "        all_preds.extend(outputs.cpu().squeeze().tolist())\n",
        "        all_targets.extend(targets.cpu().squeeze().tolist())\n",
        "\n",
        "# Compute RÂ² score\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "\n",
        "# Plot predictions vs ground truth\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(all_targets, all_preds, color='blue')\n",
        "plt.plot([min(all_targets), max(all_targets)], [min(all_targets), max(all_targets)], 'r--')\n",
        "plt.xlabel(\"Ground Truth\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Predicted vs Ground Truth Cell Counts\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GLt-dg6Qcbhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}